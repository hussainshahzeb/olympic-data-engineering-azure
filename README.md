# ðŸš€ Azure Data Engineering Project (End-to-End)

This repository showcases a complete, beginner-friendly **Azure Data Engineering project**, built using modern tools such as **Azure Data Factory**, **Databricks**, **PySpark**, **Azure Data Lake**, and **Azure DevOps** for CI/CD. It follows an end-to-end pipeline â€” from data ingestion to transformation and orchestration â€” with practical implementations aligned to real-world scenarios and interview preparation.

---

## ðŸ“Š Datasets Used

The project utilizes publicly available datasets related to the **Olympic Games** available on Kaggle, including:

- `athletes.csv` â€“ Information on athletes and their demographics  
- `coaches.csv` â€“ Data on coaches participating in different games  
- `events.csv` â€“ Event details including disciplines and results  
- `nocs.csv` â€“ National Olympic Committees with region codes  

These datasets are ingested, processed, and transformed to simulate a real-world data engineering workflow using Azure services.

---

## ðŸ”§ Technologies Used

- **Azure Data Factory (ADF)**
- **Azure Data Lake Storage (ADLS Gen2)**
- **Azure Databricks** + **Unity Catalog**
- **Apache Spark** / **PySpark**
- **Delta Live Tables (DLT)**
- **Azure DevOps** (CI/CD, Git, Pipelines)
- **Structured Streaming** & **Change Data Capture (CDC)**

---

## âœ… Key Features

- Cloud resource setup via Azure Free Account
- Git integration & CI/CD pipelines using Azure DevOps
- Data ingestion from GitHub using Azure Data Factory
- Parametrized ETL pipelines with ForEach activity
- Data transformation using PySpark in Databricks
- Orchestration using Databricks Workflows
- Real-time processing with Structured Streaming & CDC
- Deployment via ARM templates and pull requests
